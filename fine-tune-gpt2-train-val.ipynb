{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Ubuntu Automated Customer Service"]},{"cell_type":"markdown","metadata":{},"source":["Project Overview\n","\n","This project is focused on creating a conversational AI system designed to automate customer service for Ubuntu users. By leveraging the Ubuntu Dialogue Corpus, the system will be trained to understand customer queries and offer automated solutions. The project encompasses several key phases, including data preprocessing, development and training of a natural language processing (NLP) model, and the integration of this model into a chatbot interface that users can interact with.\n","\n","Data Source:\n","- The dataset for this project, the Ubuntu Dialogue Corpus, is available on Kaggle. [link](https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus)\n","\n","Project Goals:\n","- To preprocess the Ubuntu Dialogue Corpus data for NLP.\n","- To build and train an NLP model capable of understanding and responding to user queries.\n","- To integrate the trained NLP model into a chatbot interface for automated customer service.\n","- To evaluate the effectiveness and accuracy of the conversational AI system in handling real-world user queries.\n","\n","Steps:\n","\n","1. **Data Acquisition and Preprocessing**:\n","- Download the Ubuntu Dialogue Corpus from Kaggle.\n","- Clean and preprocess the data to format it suitably for NLP tasks. This may include tokenization, removing stop words, and stemming or lemmatization.\n","\n","2. **Model Development**:\n","- Select an appropriate NLP model architecture that can process the conversational data effectively. This could involve sequence-to-sequence models, transformers, or other architectures suitable for handling dialogue.\n","- Implement the model using a machine learning framework such as TensorFlow or PyTorch.\n","\n","3. **Training**:\n","- Train the model on the preprocessed Ubuntu Dialogue Corpus, adjusting parameters and structures as necessary to improve performance.\n","- Use a portion of the data for validation to monitor the model's performance and prevent overfitting.\n","\n","4. **Chatbot Integration**:\n","- Develop a chatbot interface that can interact with users in real-time. This interface should be capable of processing user inputs, passing them to the trained NLP model, and displaying the model's responses.\n","- Ensure the chatbot interface is user-friendly and can handle a variety of query types.\n","\n","5. **Evaluation and Testing**:\n","- Test the conversational AI system with a set of predefined queries to assess its response accuracy and relevance.\n","- Optionally, conduct user testing with real users to gather feedback on the system's performance and identify areas for improvement.\n","\n","6. **Iteration and Improvement**:\n","- Based on testing feedback and performance evaluations, make necessary adjustments to the model and chatbot interface.\n","- Explore advanced NLP techniques and model architectures to enhance the system's understanding and response capabilities."]},{"cell_type":"markdown","metadata":{},"source":["# Step 1: Data Acquisition and Preprocessing"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T06:26:13.126122Z","iopub.status.busy":"2024-07-12T06:26:13.125705Z","iopub.status.idle":"2024-07-12T06:26:14.585244Z","shell.execute_reply":"2024-07-12T06:26:14.583978Z","shell.execute_reply.started":"2024-07-12T06:26:13.126086Z"},"trusted":true},"outputs":[],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset\n","\n","import pandas as pd\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["transformers version: 4.43.3\n","torch version: 2.4.0+cpu\n","pandas version: 2.2.2\n"]}],"source":["import transformers\n","# import torch\n","\n","print(\"transformers version:\", transformers.__version__)\n","print(\"torch version:\", torch.__version__)\n","print(\"pandas version:\", pd.__version__)\n"]},{"cell_type":"markdown","metadata":{},"source":["1.1- load data"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T06:26:14.588888Z","iopub.status.busy":"2024-07-12T06:26:14.587915Z","iopub.status.idle":"2024-07-12T06:26:14.815491Z","shell.execute_reply":"2024-07-12T06:26:14.814132Z","shell.execute_reply.started":"2024-07-12T06:26:14.588760Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hello folks, please help me a bit with the fol...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Did I choose a bad channel? I ask because you ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>the second sentence is better english   and we...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sock Puppe?t</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>WTF?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text\n","0  Hello folks, please help me a bit with the fol...\n","1  Did I choose a bad channel? I ask because you ...\n","2  the second sentence is better english   and we...\n","3                                       Sock Puppe?t\n","4                                               WTF?"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["def load_data():\n","    \"\"\"\n","    Load the Ubuntu Dialogue Corpus data from CSV files and concatenate them into a single DataFrame.\n","    \n","    Returns:\n","        df (pd.DataFrame): Concatenated DataFrame containing dialogue texts.\n","    \"\"\"\n","    df2 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText.csv', nrows=1000)\n","    df3 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_301.csv', nrows=1000)\n","    df4 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_196.csv', nrows=1000)\n","    df = pd.concat([df2, df3, df4], ignore_index=True)\n","    df = df.drop(['folder', 'dialogueID', 'date', 'from', 'to'], axis=1)\n","    return df\n","df = load_data()\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["split for train and val"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2100,) (900,)\n"]}],"source":["train_texts, val_texts = train_test_split(df['text'], test_size=0.3)\n","print(train_texts.shape, val_texts.shape)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["train_texts_list = train_texts.tolist()\n","val_texts_list = val_texts.tolist()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["train_texts_list = [text if isinstance(text, str) else \"\" for text in train_texts_list]\n","val_texts_list = [text if isinstance(text, str) else \"\" for text in val_texts_list]"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["[' only got Memtest and memtest serial',\n"," 'Ubuntu 12.10 \\\\n \\\\l',\n"," 'there are other people wnating help']"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["train_texts_list[:3]"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["[\"you /think/ you can access the data, that doesn't mean its all there and ok\",\n"," 'graveman?',\n"," 'what video card']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["val_texts_list[:3]"]},{"cell_type":"markdown","metadata":{},"source":["# Stpe 2: Load GPT-2 from Local Directory"]},{"cell_type":"markdown","metadata":{},"source":["load tokenizer"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizer vocab size: 50258\n"]}],"source":["model_path = r\"saved_models/gpt2\" \n","tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","vocab_size = len(tokenizer)\n","print(f\"Tokenizer vocab size: {vocab_size}\")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["train_encodings = tokenizer(train_texts_list, truncation=True, padding=True, return_tensors='pt')\n","val_encodings = tokenizer(val_texts_list, truncation=True, padding=True, return_tensors='pt')"]},{"cell_type":"markdown","metadata":{},"source":["load model"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["Embedding(50258, 768)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["model = GPT2LMHeadModel.from_pretrained(model_path)\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","metadata":{},"source":["# Step 3: Prepare Your Dataset\n","\n","Convert your dataset (which has a single column 'text') into the format required for training."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2100 900\n"]}],"source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __len__(self):\n","        return len(self.encodings.input_ids)\n","\n","    def __getitem__(self, idx):\n","        item = {\n","            'input_ids': self.encodings.input_ids[idx],\n","            'attention_mask': self.encodings.attention_mask[idx],\n","            'labels': self.encodings.input_ids[idx]  # Use the input_ids as labels for language modeling\n","        }\n","        return item\n","\n","\n","train_dataset = CustomDataset(train_encodings)\n","val_dataset = CustomDataset(val_encodings)\n","print(train_dataset.__len__(), val_dataset.__len__())"]},{"cell_type":"markdown","metadata":{},"source":["# Step 4: Fine-Tune"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\CENTER_ELRahama\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainingArguments(\n","_n_gpu=0,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=10,\n","eval_strategy=IntervalStrategy.STEPS,\n","eval_use_gather_object=False,\n","evaluation_strategy=steps,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.001,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=./logs,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=10,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=2,\n","optim=OptimizerNames.ADAMW_TORCH,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=./results,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=32,\n","per_device_train_batch_size=32,\n","prediction_loss_only=True,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=[],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=./results,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=2,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.01,\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["training_args = TrainingArguments(\n","    output_dir='./results',\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=2,\n","    learning_rate=0.001,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    evaluation_strategy=\"steps\",\n","    save_steps=500,\n","    fp16=False,\n","    save_total_limit=2,\n","    prediction_loss_only=True  # Ensure loss is calculated\n",")\n","\n","training_args"]},{"cell_type":"markdown","metadata":{},"source":["Start fine-tuning"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/132 [1:49:01<?, ?it/s]\n","                                       \n","  0%|          | 0/132 [56:44<?, ?it/s]           "]},{"name":"stdout","output_type":"stream","text":["{'loss': 9.0177, 'grad_norm': 8.53919506072998, 'learning_rate': 0.0009242424242424242, 'epoch': 0.15}\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","                                       \n","\u001b[A                                               \n","\n","  0%|          | 0/132 [1:05:20<?, ?it/s]      \n","\u001b[A\n","\u001b[A"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.6604344844818115, 'eval_runtime': 516.0397, 'eval_samples_per_second': 1.744, 'eval_steps_per_second': 0.056, 'epoch': 0.15}\n"]},{"name":"stderr","output_type":"stream","text":["                                         \n","  0%|          | 0/132 [1:15:29<?, ?it/s]           "]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.066, 'grad_norm': 0.3350197970867157, 'learning_rate': 0.0008484848484848485, 'epoch': 0.3}\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","                                         \n","\u001b[A                                                 \n","\n","  0%|          | 0/132 [1:24:07<?, ?it/s]      \n","\u001b[A\n","\u001b[A"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.9315052032470703, 'eval_runtime': 517.8907, 'eval_samples_per_second': 1.738, 'eval_steps_per_second': 0.056, 'epoch': 0.3}\n"]},{"name":"stderr","output_type":"stream","text":[]}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset\n",")\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["# Step 5: Save the fine-tuned model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save_pretrained('./fine-tuned-gpt2')\n","tokenizer.save_pretrained('./fine-tuned-gpt2')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1982,"sourceId":3401,"sourceType":"datasetVersion"},{"datasetId":5357112,"sourceId":8909594,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
