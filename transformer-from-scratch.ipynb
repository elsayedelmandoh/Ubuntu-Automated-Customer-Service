{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Ubuntu Automated Customer Service"]},{"cell_type":"markdown","metadata":{},"source":["Project Overview\n","\n","This project is focused on creating a conversational AI system designed to automate customer service for Ubuntu users. By leveraging the Ubuntu Dialogue Corpus, the system will be trained to understand customer queries and offer automated solutions. The project encompasses several key phases, including data preprocessing, development and training of a natural language processing (NLP) model, and the integration of this model into a chatbot interface that users can interact with.\n","\n","Data Source:\n","- The dataset for this project, the Ubuntu Dialogue Corpus, is available on Kaggle. [Link](https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus)\n","\n","Project Goals:\n","- To preprocess the Ubuntu Dialogue Corpus data for NLP.\n","- To build and train an NLP model capable of understanding and responding to user queries.\n","- To integrate the trained NLP model into a chatbot interface for automated customer service.\n","- To evaluate the effectiveness and accuracy of the conversational AI system in handling real-world user queries.\n","\n","Steps:\n","\n","1. **Data Acquisition and Preprocessing**:\n","- Download the Ubuntu Dialogue Corpus from Kaggle.\n","- Clean and preprocess the data to format it suitably for NLP tasks. This may include tokenization, removing stop words, and stemming or lemmatization.\n","\n","2. **Model Development**:\n","- Select an appropriate NLP model architecture that can process the conversational data effectively. This could involve sequence-to-sequence models, transformers, or other architectures suitable for handling dialogue.\n","- Implement the model using a machine learning framework such as TensorFlow or PyTorch.\n","\n","3. **Training**:\n","- Train the model on the preprocessed Ubuntu Dialogue Corpus, adjusting parameters and structures as necessary to improve performance.\n","- Use a portion of the data for validation to monitor the model's performance and prevent overfitting.\n","\n","4. **Chatbot Integration**:\n","- Develop a chatbot interface that can interact with users in real-time. This interface should be capable of processing user inputs, passing them to the trained NLP model, and displaying the model's responses.\n","- Ensure the chatbot interface is user-friendly and can handle a variety of query types.\n","\n","5. **Evaluation and Testing**:\n","- Test the conversational AI system with a set of predefined queries to assess its response accuracy and relevance.\n","- Optionally, conduct user testing with real users to gather feedback on the system's performance and identify areas for improvement.\n","\n","6. **Iteration and Improvement**:\n","- Based on testing feedback and performance evaluations, make necessary adjustments to the model and chatbot interface.\n","- Explore advanced NLP techniques and model architectures to enhance the system's understanding and response capabilities."]},{"cell_type":"markdown","metadata":{},"source":["# Step 1: Data Acquisition and Preprocessing"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:02:09.814912Z","iopub.status.busy":"2024-07-09T09:02:09.814560Z","iopub.status.idle":"2024-07-09T09:02:10.192533Z","shell.execute_reply":"2024-07-09T09:02:10.191796Z","shell.execute_reply.started":"2024-07-09T09:02:09.814886Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["1.1- load data"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:04:01.482046Z","iopub.status.busy":"2024-07-09T09:04:01.481680Z","iopub.status.idle":"2024-07-09T09:04:02.032052Z","shell.execute_reply":"2024-07-09T09:04:02.031101Z","shell.execute_reply.started":"2024-07-09T09:04:01.482019Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hello folks, please help me a bit with the fol...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Did I choose a bad channel? I ask because you ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>the second sentence is better english   and we...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sock Puppe?t</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>WTF?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text\n","0  Hello folks, please help me a bit with the fol...\n","1  Did I choose a bad channel? I ask because you ...\n","2  the second sentence is better english   and we...\n","3                                       Sock Puppe?t\n","4                                               WTF?"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["def load_data():\n","    # Load your data\n","    df2 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText.csv', nrows=10)\n","    df3 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_301.csv', nrows=10)\n","    df4 = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_196.csv', nrows=10)\n","    df = pd.concat([df2, df3, df4], ignore_index=True)\n","    df = df.drop(['folder', 'dialogueID', 'date', 'from', 'to'], axis=1)\n","    return df\n","df = load_data()\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["1.2 Data Inspection:\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:06:05.509913Z","iopub.status.busy":"2024-07-09T09:06:05.509533Z","iopub.status.idle":"2024-07-09T09:06:05.544112Z","shell.execute_reply":"2024-07-09T09:06:05.543189Z","shell.execute_reply.started":"2024-07-09T09:06:05.509884Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 30 entries, 0 to 29\n","Data columns (total 1 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   text    30 non-null     object\n","dtypes: object(1)\n","memory usage: 372.0+ bytes\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{},"source":["1.3 Data Preprocessing:\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:08:56.634883Z","iopub.status.busy":"2024-07-09T09:08:56.634503Z","iopub.status.idle":"2024-07-09T09:09:09.882530Z","shell.execute_reply":"2024-07-09T09:09:09.881404Z","shell.execute_reply.started":"2024-07-09T09:08:56.634852Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\CENTER_ELRahama\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\CENTER_ELRahama\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\CENTER_ELRahama\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["# !pip install nltk\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","#nltk.download()\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","nltk.download('wordnet')\n","stemmer= PorterStemmer()    \n","lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:09:43.382515Z","iopub.status.busy":"2024-07-09T09:09:43.381932Z","iopub.status.idle":"2024-07-09T09:10:31.792195Z","shell.execute_reply":"2024-07-09T09:10:31.791264Z","shell.execute_reply.started":"2024-07-09T09:09:43.382484Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>cleaned_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hello folks, please help me a bit with the fol...</td>\n","      <td>hello folk pleas help bit follow sentenc order...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Did I choose a bad channel? I ask because you ...</td>\n","      <td>choos bad channel ask seem dumb like window user</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>the second sentence is better english   and we...</td>\n","      <td>second sentenc better english dumb</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sock Puppe?t</td>\n","      <td>sock puppet</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>WTF?</td>\n","      <td>wtf</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0  Hello folks, please help me a bit with the fol...   \n","1  Did I choose a bad channel? I ask because you ...   \n","2  the second sentence is better english   and we...   \n","3                                       Sock Puppe?t   \n","4                                               WTF?   \n","\n","                                        cleaned_text  \n","0  hello folk pleas help bit follow sentenc order...  \n","1   choos bad channel ask seem dumb like window user  \n","2                 second sentenc better english dumb  \n","3                                        sock puppet  \n","4                                                wtf  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["def preprocess_text(text):\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n","    tokens = word_tokenize(text)\n","    stemmered = [\n","        stemmer.stem(token)\n","        for token in tokens \n","        if (not token.isnumeric()) and (len(token) > 2) and (token not in stop_words)\n","    ]\n","    return \" \".join(stemmered)\n","\n","df['cleaned_text'] = df['text'].apply(preprocess_text)\n","df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:11:24.075231Z","iopub.status.busy":"2024-07-09T09:11:24.074877Z","iopub.status.idle":"2024-07-09T09:11:24.154726Z","shell.execute_reply":"2024-07-09T09:11:24.153793Z","shell.execute_reply.started":"2024-07-09T09:11:24.075202Z"},"trusted":true},"outputs":[{"data":{"text/plain":["text            0\n","cleaned_text    0\n","dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df.dropna(inplace=True)\n","df.isna().sum()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:17:32.565285Z","iopub.status.busy":"2024-07-09T09:17:32.564452Z","iopub.status.idle":"2024-07-09T09:17:32.579042Z","shell.execute_reply":"2024-07-09T09:17:32.578112Z","shell.execute_reply.started":"2024-07-09T09:17:32.565252Z"},"trusted":true},"outputs":[],"source":["input_texts = df['cleaned_text'].tolist()[:-1]\n","target_texts = df['cleaned_text'].tolist()[1:]"]},{"cell_type":"markdown","metadata":{},"source":["Transformers, like many sequence-to-sequence models, need to be trained with input sequences that predict the next word in a sequence. This requires a specific way of preparing the target sequences:\n","\n","- Input Sequences: The input sequences remain as they are.\n","\n","- Response Sequences: These sequences are used to create both the decoder input and the decoder output sequences.\n","\n","    + Decoder Input: The target sequences are shifted right by one position. This means that the model sees the start of a sentence and predicts the next word at each step.\n","\n","    + Decoder Output: This is the actual target sequence which the model should predict.\n","\n","Here's how this looks in practice:\n","\n","For a given target sequence: [START, How, are, you, doing, ?]\n","\n","Decoder Input: [START, How, are, you, doing]\n","\n","Decoder Output: [How, are, you, doing, ?]"]},{"cell_type":"markdown","metadata":{},"source":["- 1.4- `Tokenization` using Tokenizer class and fit_on_texts method\n","\n","    - Convert texts to tokens \n","\n","- 1.5- `Sequences` using texts_to_sequences method\n","\n","    - Convert tokens to Sequences, return sequences\n","\n","- 1.6- `Padding` using pad_sequences method\n","\n","    - Convert Sequences to pad sequences\n","    - Ensuring all sequences have the same length.\n","    - Return pad_sequences\n","\n","- 1.7-  `split 'pad_sequences' array` to input_pad_sequences and target_pad_sequences\n","    - If we build transformer\n","    - For prepare Decoder Input and Output Sequences\n","\n","- 1.8- `Split data` using train_test_split method\n","\n","    - Perform the train-test split on the padded sequences with "]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:19:10.168907Z","iopub.status.busy":"2024-07-09T09:19:10.168250Z","iopub.status.idle":"2024-07-09T09:19:30.970293Z","shell.execute_reply":"2024-07-09T09:19:30.969273Z","shell.execute_reply.started":"2024-07-09T09:19:10.168873Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of input train: (23, 50) and input val: (6, 50)\n","Shape of target train input: (23, 49) and target val input : (6, 49)\n","Shape of target val input: (23, 49) and target val output: (6, 49)\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","# Convert texts to tokens \n","max_vocab_size = 20000\n","tokenizer = Tokenizer(num_words=max_vocab_size,\n","                      oov_token='<OOV>') # oov to handling unknown word\n","tokenizer.fit_on_texts(input_texts + target_texts)\n","\n","# Convert tokens to Sequences\n","input_sequences = tokenizer.texts_to_sequences(input_texts)\n","target_sequences = tokenizer.texts_to_sequences(target_texts)\n","\n","# Convert sequences to padded sequences\n","max_sequence_length = 50\n","input_pad_sequences = pad_sequences(input_sequences, \n","                                maxlen=max_sequence_length, \n","                                padding='post')\n","target_pad_sequences = pad_sequences(target_sequences, \n","                                maxlen=max_sequence_length, \n","                                padding='post')\n","\n","# Split 'target_pad_sequences' to prepare Decoder Input and Output Sequences\n","target_pad_sequences_input = target_pad_sequences[:, :-1]\n","target_pad_sequences_output = target_pad_sequences[:, 1:]\n","\n","# Split data for training and validation\n","input_train, input_val, target_train_input, target_val_input, target_train_output, target_val_output = train_test_split(\n","    input_pad_sequences, \n","    target_pad_sequences_input,\n","    target_pad_sequences_output,\n","    test_size=0.2,\n","    random_state=42\n",")\n","\n","print(f\"Shape of input train: {input_train.shape} and input val: {input_val.shape}\")\n","print(f\"Shape of target train input: {target_train_input.shape} and target val input : {target_val_input.shape}\")\n","print(f\"Shape of target val input: {target_train_output.shape} and target val output: {target_val_output.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Step 2: Model Development"]},{"cell_type":"markdown","metadata":{},"source":["2.2- Define the Model Architecture and Implementation for automated chatbot customer service:\n","\n","2.2.1- Encoder: Processes the input text.\n","\n","- Input: Tokenized and padded input sequences.\n","- Transformer Encoder Layers: Stack multiple layers of transformer encoder blocks.\n","- Each encoder block consists of:\n","    - Multi-head self-attention mechanism.\n","    - Feedforward neural network.\n","\n","2.2.2- Decoder: Generates the response text.\n","\n","- Input: Tokenized and padded target sequences (shifted right).\n","- Transformer Decoder Layers: Stack multiple layers of transformer decoder blocks.\n","- Each decoder block consists of:\n","    - Multi-head self-attention mechanism (for target).\n","    - Multi-head attention over the encoder outputs.\n","    - Feedforward neural network.\n","- Output: Probability distribution over the vocabulary for generating responses.\n","\n","2.2.3- Attention Mechanism: Improves the performance by allowing the decoder to focus on relevant parts of the input sequence.\n","\n","- Used in both encoder and decoder to focus on relevant parts of the input sequence and improve context understanding."]},{"cell_type":"markdown","metadata":{},"source":["\n","Import necessary libraries:\n"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{},"source":["Define model parameters:\n"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:20:05.177992Z","iopub.status.busy":"2024-07-09T09:20:05.177041Z","iopub.status.idle":"2024-07-09T09:20:06.385558Z","shell.execute_reply":"2024-07-09T09:20:06.384694Z","shell.execute_reply.started":"2024-07-09T09:20:05.177960Z"},"trusted":true},"outputs":[],"source":["vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n","embed_dim = 256\n","num_heads = 8\n","ff_dim = 512\n","num_transformer_blocks = 4\n","dropout_rate = 0.1"]},{"cell_type":"markdown","metadata":{},"source":["Create the transformer block:"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["def transformer_block(inputs, num_heads, ff_dim, dropout_rate=0.1):\n","    attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n","    attention_output = layers.Dropout(dropout_rate)(attention_output)\n","    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n","    \n","    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(out1)\n","    ffn_output = layers.Dense(embed_dim)(ffn_output)\n","    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n","    \n","    return layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)"]},{"cell_type":"markdown","metadata":{},"source":["\n","Build the encoder:\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["def build_encoder(vocab_size, embed_dim, num_heads, ff_dim, num_transformer_blocks, max_sequence_length, dropout_rate=0.1):\n","    inputs = layers.Input(shape=(max_sequence_length,))\n","    embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","    x = embedding_layer(inputs)\n","    x = layers.Dropout(dropout_rate)(x)\n","    \n","    for _ in range(num_transformer_blocks):\n","        x = transformer_block(x, num_heads, ff_dim, dropout_rate)\n","    \n","    return keras.Model(inputs=inputs, outputs=x, name=\"encoder\")"]},{"cell_type":"markdown","metadata":{},"source":["Build the decoder:\n"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["def build_decoder(vocab_size, embed_dim, num_heads, ff_dim, num_transformer_blocks, max_sequence_length, dropout_rate=0.1):\n","    inputs = layers.Input(shape=(max_sequence_length,))\n","    embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","    x = embedding_layer(inputs)\n","    x = layers.Dropout(dropout_rate)(x)\n","    \n","    for _ in range(num_transformer_blocks):\n","        x = transformer_block(x, num_heads, ff_dim, dropout_rate)\n","    \n","    x = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","    \n","    return keras.Model(inputs=inputs, outputs=x, name=\"decoder\")"]},{"cell_type":"markdown","metadata":{},"source":["Build the full transformer model:"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["def build_transformer(vocab_size, embed_dim, num_heads, ff_dim, num_transformer_blocks, max_sequence_length, dropout_rate=0.1):\n","    encoder_inputs = layers.Input(shape=(max_sequence_length,), name=\"encoder_inputs\")\n","    decoder_inputs = layers.Input(shape=(max_sequence_length - 1,), name=\"decoder_inputs\")\n","    \n","    encoder = build_encoder(vocab_size, embed_dim, num_heads, ff_dim, num_transformer_blocks, max_sequence_length, dropout_rate)\n","    decoder = build_decoder(vocab_size, embed_dim, num_heads, ff_dim, num_transformer_blocks, max_sequence_length - 1, dropout_rate)\n","    \n","    encoder_outputs = encoder(encoder_inputs)\n","    decoder_outputs = decoder(decoder_inputs)\n","    \n","    outputs = layers.Dense(vocab_size, activation=\"softmax\")(decoder_outputs)\n","    \n","    model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs, name=\"transformer\")\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["Create and compile the model:"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," decoder_inputs (InputLayer  [(None, 49)]                 0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," decoder (Functional)        (None, 49, 75)               9508427   ['decoder_inputs[0][0]']      \n","                                                                                                  \n"," encoder_inputs (InputLayer  [(None, 50)]                 0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," dense_109 (Dense)           (None, 49, 75)               5700      ['decoder[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 9514127 (36.29 MB)\n","Trainable params: 9514127 (36.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["model = build_transformer(vocab_size, embed_dim, num_heads, ff_dim, num_transformer_blocks, max_sequence_length, dropout_rate)\n","\n","model.compile(\n","    optimizer=\"adam\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# Step 3: Training, validation and testing"]},{"cell_type":"markdown","metadata":{},"source":["Train the model"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:20:48.261661Z","iopub.status.busy":"2024-07-09T09:20:48.260973Z","iopub.status.idle":"2024-07-09T09:27:32.285294Z","shell.execute_reply":"2024-07-09T09:27:32.284472Z","shell.execute_reply.started":"2024-07-09T09:20:48.261617Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","1/1 [==============================] - 1s 1s/step - loss: 4.1229 - accuracy: 0.9556 - val_loss: 4.1221 - val_accuracy: 0.9422\n","Epoch 2/3\n","1/1 [==============================] - 1s 930ms/step - loss: 4.1195 - accuracy: 0.9556 - val_loss: 4.1187 - val_accuracy: 0.9422\n","Epoch 3/3\n","1/1 [==============================] - 1s 848ms/step - loss: 4.1161 - accuracy: 0.9556 - val_loss: 4.1152 - val_accuracy: 0.9422\n","1/1 [==============================] - 0s 106ms/step - loss: 4.1152 - accuracy: 0.9422\n","Validation Loss: 4.115200042724609\n","Validation Accuracy: 0.942176878452301\n"]}],"source":["history = model.fit(\n","    [input_train, target_train_input],\n","    target_train_output,\n","    validation_data=([input_val, target_val_input], target_val_output),\n","    epochs=3,\n","    batch_size=64\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Evaluate model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_loss, val_accuracy = model.evaluate([input_val, target_val_input], target_val_output)\n","print(f\"Validation Loss: {val_loss}\")\n","print(f\"Validation Accuracy: {val_accuracy}\")"]},{"cell_type":"markdown","metadata":{},"source":["Test model"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: How can I install Ubuntu?\n","Response: jdk ibm get plugin help prgidi pleas load prdigi blackdown tri person prgidi tri idea java folk bad ye help afaik blackdown channel noneu pleas sun yet bit person yet follow better java prodigi onlin doesnt dumb somewher jdk noneu hello think <OOV> bad ibm guy develop second\n"]}],"source":["def generate_response(model, input_sequence, tokenizer, max_sequence_length, temperature=1.0):\n","    # Tokenize and pad the input sequence\n","    input_seq = tokenizer.texts_to_sequences([input_sequence])\n","    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n","\n","    # Initialize the target sequence with the start token\n","    target_seq = np.zeros((1, max_sequence_length - 1))\n","    target_seq[0, 0] = tokenizer.word_index.get('<start>', 0)  # Assuming '<start>' token exists\n","\n","    # Generate the response\n","    decoded_sentence = []\n","    for i in range(max_sequence_length - 2):  # Changed this to avoid index out of bounds\n","        # Predict the next token\n","        output = model.predict([input_seq, target_seq], verbose=0)\n","        sampled_token_index = sample_token(output[0, i, :], temperature)\n","        sampled_word = tokenizer.index_word.get(sampled_token_index, '<UNK>')\n","\n","        # Exit condition: either hit max length or find the end token\n","        if sampled_word == '<end>' or len(decoded_sentence) >= max_sequence_length - 2:\n","            break\n","\n","        decoded_sentence.append(sampled_word)\n","\n","        # Update the target sequence for the next iteration\n","        if i < max_sequence_length - 2:  # Ensure we don't go out of bounds\n","            target_seq[0, i+1] = sampled_token_index\n","\n","    return ' '.join(decoded_sentence)\n","\n","def sample_token(probabilities, temperature=1.0):\n","    # Apply temperature to the probabilities\n","    probabilities = np.asarray(probabilities).astype('float64')\n","    probabilities = np.log(probabilities) / temperature\n","    exp_probabilities = np.exp(probabilities)\n","    probabilities = exp_probabilities / np.sum(exp_probabilities)\n","    \n","    # Sample from the distribution\n","    return np.random.choice(len(probabilities), p=probabilities)\n","\n","# Example usage:\n","input_text = \"How can I install Ubuntu?\"\n","response = generate_response(model, input_text, tokenizer, max_sequence_length)\n","print(f\"Input: {input_text}\")\n","print(f\"Response: {response}\")"]},{"cell_type":"markdown","metadata":{},"source":["Save the trained model and tokenizer"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T09:36:09.167369Z","iopub.status.busy":"2024-07-09T09:36:09.166675Z","iopub.status.idle":"2024-07-09T09:36:09.602640Z","shell.execute_reply":"2024-07-09T09:36:09.601706Z","shell.execute_reply.started":"2024-07-09T09:36:09.167338Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model and tokenizer saved in /kaggle/working/saved_models\n"]}],"source":["import pickle\n","import os\n","\n","def save_model_and_tokenizer(model, tokenizer, save_dir):\n","    model.save(os.path.join(save_dir, 'chatbot_model.h5'))\n","    with open(os.path.join(save_dir, 'tokenizer.pickle'), 'wb') as handle:\n","        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    print(f\"Model and tokenizer saved in {save_dir}\")\n","\n","save_dir = '/kaggle/working/saved_models'\n","os.makedirs(save_dir, exist_ok=True)\n","save_model_and_tokenizer(model, tokenizer, save_dir)"]},{"cell_type":"markdown","metadata":{},"source":["# Step 4: Chatbot Integration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Step 5: Evaluation and Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Step 6. Iteration and Improvement"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5357112,"sourceId":8909594,"sourceType":"datasetVersion"},{"datasetId":1982,"sourceId":3401,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
